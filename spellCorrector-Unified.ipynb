{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spellCorrect:\n",
    "    \n",
    "    def _loadData(self):\n",
    "        \"\"\"\n",
    "        Load data file in the .npy file extension.\n",
    "        \n",
    "        Input - none\n",
    "        Output - words, bgrms, tgrms\n",
    "        \"\"\"\n",
    "        words = list()\n",
    "\n",
    "        wordFiles = glob(os.getcwd()+\"/Data/Processed/*/*Words.npy\")\n",
    "\n",
    "        for d in wordFiles:\n",
    "            file = np.load(d, allow_pickle=True)\n",
    "            words.append([w for w in file])\n",
    "        words = [x for sl in words for x in sl]\n",
    "\n",
    "        bgrms = nltk.bigrams(words)\n",
    "        tgrms = nltk.trigrams(words)\n",
    "        return Counter(words), Counter(bgrms), Counter(tgrms)\n",
    "        \n",
    "    def _prob(self, word):\n",
    "        \"\"\"\n",
    "        Calculate word probability based on word frequency in the corpus.\n",
    "        \n",
    "        Input - word (str)\n",
    "        Output - prob (float)\n",
    "        \"\"\"\n",
    "        prob = self.wordDict[word]/sum(self.wordDict.values())\n",
    "        return prob    \n",
    "    \n",
    "    def _edit1(self, word):\n",
    "        \"\"\"\n",
    "        Generate strings with edit distance 1 from the source word.\n",
    "        Edit distance used here is the Damerau-Levenshtein Distance.\n",
    "        \n",
    "        Input - word (str)\n",
    "        Output - set of word (str)\n",
    "        \"\"\"\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def _edit2(self, word):\n",
    "        \"\"\"\n",
    "        Generate strings with edit distance 2 from the source word.\n",
    "        Calculated based on strings with another edit distance of 1 from strings with edit distance of 1 from the source word.\n",
    "        \n",
    "        Input - word (str)\n",
    "        Output - dict of word (str)\n",
    "        \"\"\"\n",
    "        return (e2 for e1 in self._edit1(word) for e2 in self._edit1(e1))\n",
    "\n",
    "    def _known(self, words):\n",
    "        \"\"\"\n",
    "        Filter the set of strings to that which only exists in the list of known words.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if w in self.wordDict)    \n",
    "    \n",
    "    #def _GUI():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonWord(spellCorrect):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wordDict, _, _ = self._loadData()\n",
    "        \n",
    "        self.vocabSize = len(self.wordDict)\n",
    "        self.totalWord = sum(self.wordDict.values())\n",
    "        \n",
    "        self.textOut  = list()\n",
    "        \n",
    "    def _candidate(self, word):\n",
    "        \"\"\"\n",
    "        Generate set of candidate words with edit distances 1 and 2 based on the source word.\n",
    "        \n",
    "        Input - word(str)\n",
    "        Output - list of set of word (str)\n",
    "        \"\"\"\n",
    "        return (self._known([word]) or            # if the word is known (no error),\n",
    "                self._known(self._edit1(word)) or # if the word has edit distance 1,\n",
    "                self._known(self._edit2(word)) or # if the word has edit distance 2,\n",
    "                {word})                           # if the word cannot be found from any of the above.\n",
    "\n",
    "    def _correct(self, word):\n",
    "        return max(self._candidate(word), key=self._prob)\n",
    "    \n",
    "    def correct(self, textInput):\n",
    "        textInput = textInput.group()\n",
    "        return ''.join(self._correct(textInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class realWord(spellCorrect):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wordDict, self.bgrmDict, self.tgrmDict = self._loadData()\n",
    "        \n",
    "        self.vocabSize = len(self.wordDict)\n",
    "        self.totalWord = sum(self.wordDict.values())\n",
    "        \n",
    "        self.textOut = list()\n",
    "\n",
    "    def _wordCandidate(self, word):\n",
    "        \"\"\"\n",
    "        Generate set of candidate words edit distance 1 based on the source word.\n",
    "        \n",
    "        Input - word(str)\n",
    "        Output - list of set of word (str)\n",
    "        \"\"\"\n",
    "        candidates = [self._known([word]),            # if the word is known (no error),\n",
    "                      self._known(self._edit1(word)), # if the word has edit distance 1,\n",
    "                      #self._known(self._edit2(word)),# if the word has edit distance 2,\n",
    "                      {word}]                         # if the word cannot be found from any of the above.\n",
    "        return set([x for sl in candidates for x in sl]) # return a set of all unique known words in the case of real-word correction\n",
    "    \n",
    "    def _sentProb(self, sent):\n",
    "        \"\"\"\n",
    "        Calculate sentence probability based on a combination of bigram and trigram frequency in the corpus.\n",
    "        \n",
    "        Input - list of words (str)\n",
    "        Output - prob (float)\n",
    "        \"\"\"\n",
    "        sent = sent.split()\n",
    "        prob = self._prob(sent[0])\n",
    "        for i in range(1, len(sent)-1):\n",
    "            P1 = self.bgrmDict[(sent[i-1], sent[i])] * self.wordDict[sent[i]] / self.wordDict[sent[i-1]]\n",
    "            P2 = self.bgrmDict[(sent[i], sent[i+1])] * self.wordDict[sent[i]] / self.wordDict[sent[i+1]]\n",
    "            P3 = self.tgrmDict[(sent[i-1], sent[i], sent[i+1])] * self.wordDict[sent[i]] / (self.wordDict[sent[i-1]] * self.wordDict[sent[i+1]])\n",
    "            P  = 0.25*P1 + 0.25*P2 + 0.5*P3\n",
    "            prob *= P\n",
    "        return prob\n",
    "    \n",
    "    def _sentCandidate(self, para):\n",
    "        \"\"\"\n",
    "        Generate set of candidate sentences based on the source sentence, \"sent\", and the corpus.\n",
    "        \n",
    "        Input - sent (str)\n",
    "        Output - list of words (str)\n",
    "        \"\"\"\n",
    "        sentList = nltk.tokenize.sent_tokenize(para)\n",
    "        sentCands = list()\n",
    "        for s in sentList:\n",
    "            wordList = s.split()\n",
    "            for i in range(len(wordList)):\n",
    "                for c in self._wordCandidate(wordList[i]):\n",
    "                    sentOrig = wordList\n",
    "                    sentOrig[i] = c\n",
    "                    sentCands.append(\" \".join([w for w in sentOrig]))\n",
    "        sentCands = pd.Series(sentCands).unique()\n",
    "        return sentCands\n",
    "    \n",
    "    def _correct(self, sent):\n",
    "        return max(self._sentCandidate(sent), key=self._sentProb)\n",
    "    \n",
    "    def correct(self, textInput):\n",
    "        textInput = textInput.group()\n",
    "        return ''.join(self._correct(textInput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonword = nonWord()\n",
    "realword = realWord()\n",
    "\n",
    "def correct(string):\n",
    "    string = re.sub(r'\\b\\w{3}\\w+\\b', nonword.correct, string)\n",
    "    string = re.sub(r'\\b\\w{3}\\w+\\b', realword.correct, string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Two NASA astronauts are preparing for a spacewalk on Tuesday to replace a faulty antenna system on the International Space Station. Flight Engineers Thomas Marshburn and Kayla Barron will exit the orbiting lab tomorrow after setting their U.S. spacesuits to battery power at 7:10 a.m. EST signifying the start of their spacewalk. The duo was joined on Monday by three of their fellow Expedition 66 flight engineers collecting tools and reviewing procedures planned for the six-and-a-half-hour spacewalk. NASA astronaut Raja Chari partnered with Marshburn and Barron gathering and organizing tethers, cameras, and pistol grip tools. The three astronauts then joined NASA astronaut Mark Vande Hei and ESA (European Space Agency) astronaut Matthias Maurer for a procedures cnference with spacewalk specialists on the ground. Chari and Vande Hei will be on duty throughout Tuesday monitoring the two astronauts during the spacewalk and helping them in and out of their spacesuits. Maurer will be at the controls of the Canadarm2 robotic arm assisting the spacewalkers at the Port-1 truss structure worksite. NASA TV begins its live coverage on Tuesday at 5:30 a.m. on the agencyâ€™s website, and the NASA app. The stationâ€™s two cosmonauts, Flight Engineer Pyotr Dubrov and Commander Anton Shkaplerov, spent their day on a variety of space research and maintenance tasks in the orbiting labâ€™s Russian segment. Dubrov photographed the condition of the Nauka multipurpose laboratory module following the Prichal moduleâ€™s docking on Friday. Shkaplerov swapped out life support hardware and began unpacking cargo from the newly arrived Prichal docking port.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = correct(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
