{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "esaTexts  = np.load(os.getcwd()+\"/Data/Raw/esablog.npy\")\n",
    "nasaTexts = np.load(os.getcwd()+\"/Data/Raw/nasablog_big.npy\")\n",
    "pediaTexts = np.load(os.getcwd()+\"/Data/Raw/encyclopedia.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "esaSents = np.load(os.getcwd()+\"/Data/Processed/ESA/esaSents.npy\", allow_pickle=True)\n",
    "esaWords = np.load(os.getcwd()+\"/Data/Processed/ESA/esaWords.npy\", allow_pickle=True)\n",
    "esa3gram = np.load(os.getcwd()+\"/Data/Processed/ESA/esa3gram.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasaSents = np.load(os.getcwd()+\"/Data/Processed/NASA/nasaSents.npy\", allow_pickle=True)\n",
    "nasaWords = np.load(os.getcwd()+\"/Data/Processed/NASA/nasaWords.npy\", allow_pickle=True)\n",
    "nasa3gram = np.load(os.getcwd()+\"/Data/Processed/NASA/nasa3gram.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcess(paras, nWords):\n",
    "    \"\"\"\n",
    "    Text Processing Pipeline\n",
    "    ------------------------\n",
    "    Pipeline that takes in a list of paragraphs outputs a list of sentences and a list of words.\n",
    "    Consists of:\n",
    "    1. Sentence Tokenizer (nltk.sent_tokenize)\n",
    "    2. Word Tokenizer (nltk.word_tokenize)\n",
    "    ------------------------\n",
    "    Inputs  - para\n",
    "    Outputs - sents, words\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = list()\n",
    "    words = list()\n",
    "    ngrms = list()\n",
    "    \n",
    "    for t in paras:\n",
    "        sents.append(sent_tokenize(t))\n",
    "    sents = [x for sl in sents for x in sl]\n",
    "    sents = [x.lstrip() for x in sents]\n",
    "    sents = pd.Series(sents).astype(str)\n",
    "    \n",
    "    for s in sents:\n",
    "        split = s.split()\n",
    "        for i in range(len(split) - nWords+1):\n",
    "            ngrms.append(split[i:i+nWords])\n",
    "            \n",
    "    for s in sents:\n",
    "        #s = s[0].lower() + s[1:]\n",
    "        s = re.findall(r'\\w+', s)\n",
    "        #s = re.sub(\"[^\\w'\\s\\-&.]+\", '', s)\n",
    "        words.append(s)#.split(\" \"))\n",
    "    words = [x for l in words for x in l]\n",
    "    words = [x for x in words if not any(s.isdigit() for s in x)]\n",
    "    #words = [re.sub(r'^[-\\'&]', '', w) for w in words] # remove words that start with \"-\", \"'\", or \"&\"\n",
    "    #words = [re.sub(r'[-\\'&]$', '', w) for w in words] # remove words that end with \"-\", \"'\", or \"&\"\n",
    "    words = pd.Series(words)\n",
    "    words = words[(words.str.len()!=1) | (words.isin(['a', 'A', 'I'])) | (words.str.isnumeric())]\n",
    "    \n",
    "    print(\"Total of\", len(words), \"words found.\")\n",
    "    return sents, ngrms, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 41784 words found.\n"
     ]
    }
   ],
   "source": [
    "s, n, w = textProcess(esaTexts, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.getcwd()+\"/Data/Processed/ESA/esaWords.npy\", w)\n",
    "np.save(os.getcwd()+\"/Data/Processed/ESA/esa3gram.npy\", n)\n",
    "np.save(os.getcwd()+\"/Data/Processed/ESA/esaSents.npy\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 352855 words found.\n"
     ]
    }
   ],
   "source": [
    "s, n, w = textProcess(nasaTexts, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.getcwd()+\"/Data/Processed/NASA/nasaWords.npy\", w)\n",
    "np.save(os.getcwd()+\"/Data/Processed/NASA/nasa3gram.npy\", n)\n",
    "np.save(os.getcwd()+\"/Data/Processed/NASA/nasaSents.npy\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 770865 words found.\n"
     ]
    }
   ],
   "source": [
    "s, n, w = textProcess(pediaTexts, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.getcwd()+\"/Data/Processed/Encyclopedia/pediaWords.npy\", w)\n",
    "np.save(os.getcwd()+\"/Data/Processed/Encyclopedia/pedia3gram.npy\", n)\n",
    "np.save(os.getcwd()+\"/Data/Processed/Encyclopedia/pediaSents.npy\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
